#
# Actions on the localhost: Bring up EC2 instances and add to our groups
#

---
#
# This playbook runs actions not on some remote server, but on the local
# host on which Ansible is running. It uses the "ec2" module to dynamically
# create hosts.
#
- name: create the cluster hosts
  hosts: localhost
  gather_facts: False

  tasks:
    - name: get unique ID for cluster
      local_action: >
        command date "+%N"
      register: timestamp

    #
    # Using the "ec2" module, we create some number of hosts. All those
    # variables here just specify the parameters, access keys, image types,
    # and so on.
    #
    # The module returns references to all the created instances. We remember
    # ("register") them in the variable "ec2results".
    #
    - name: launch ec2 instances
      local_action: >
        ec2 key_name={{ EXT.EC2.regions_settings[EXT.EC2.region].ssh_key_name }}
            group={{ EXT.EC2.regions_settings[EXT.EC2.region].security_group }}
            instance_type={{ EXT.EC2.instance_type }}
            image={{ EXT.EC2.regions_settings[EXT.EC2.region].image }}
            region={{ EXT.EC2.region }}
            count={{ EXT.ARCHITECTURE.num_hosts }}
            wait=true
      register: ec2results

    #
    # Now we dynamically add all those hosts ("with items: # ec2results.instances")
    # to the group, for whose name we just chose the EC2-region name. Since EC2
    # uses different SSH keys (and possibly even security groups) in different
    # regions, it is useful to have these hosts in a region-specific group.
    # That allows us to specify a variable file with the region-sepecific
    # settings.
    #
    - name: add new instance to ec2-hosts group
      local_action: add_host hostname={{ item.public_ip }} groupname=ec2-hosts
      with_items: ec2results.instances

    #
    # We take one (the first) of those machines to populate the frontend group.
    # I'm making sure to define the ansible_ssh_host variable for this host,
    # since we will need that variable much later, when we fill in the load
    # balancing group in the nginx config file.
    #
    - name: add new instances to frontend group
      local_action: >
        add_host hostname={{ ec2results.instances[0].public_ip }}
                 ansible_ssh_host={{ ec2results.instances[0].public_ip }}
                 groupname=frontend-hosts

    #
    # We take the second one to fill the backend group.
    #
    - name: add new instances to backend group
      local_action: >
        add_host hostname={{ ec2results.instances[1].public_ip }}
                 groupname=backend-hosts

    #
    # We also make the second one our apt-cache host.
    #
    - name: add new instances to cache group
      local_action: >
        add_host hostname={{ ec2results.instances[1].public_ip }}
                 groupname=cache
      when: cache.pkg_cache == "create"

    #
    # All the remaining hosts are added to the applayer host group.
    #
    - name: add new instances to applayer group
      local_action: >
        add_host hostname={{ item.public_ip }}
        groupname=applayer-hosts
      with_items: ec2results.instances[2:]

    #
    # We won't proceed until we have confirmed that we have SSH access to
    # each instance. It won't actually attempt to login. It just waits until
    # the port for SSH opens on the host.
    #
    - name: wait for SSH to come up
      local_action: >
        wait_for host={{ item.public_dns_name }} port=22
                 delay=10 timeout=320 state=started
      with_items: ec2results.instances

